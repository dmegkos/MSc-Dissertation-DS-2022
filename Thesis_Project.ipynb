{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data handling and EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "# For NLP and ML\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "import spacy\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the whole dataset to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations metadata csv to get label info\n",
    "annot = pd.read_csv(\"hate-speech-dataset-master/annotations_metadata.csv\")\n",
    "# Keep only file id and label\n",
    "annot.drop(columns=['user_id','subforum_id','num_contexts'],inplace=True)\n",
    "# Add new empty column to insert the message\n",
    "annot['message']=''\n",
    "# Loop through all lines\n",
    "for i in range(len(annot['file_id'])):\n",
    "    # Get current filename\n",
    "    filename = annot['file_id'][i]\n",
    "    # Open the file and read the contents\n",
    "    with open('hate-speech-dataset-master/all_files/' +filename +'.txt') as f:\n",
    "        contents= f.read()\n",
    "    f.close()\n",
    "    # Add contents to the message column\n",
    "    annot['message'][i]=contents\n",
    "# Drop file id\n",
    "annot.drop(columns=['file_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame information\n",
    "annot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the classes and their value counts\n",
    "annot['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the original paper, there are a total of 10944 sentences, 9507 of them are classified as no hate speech, 1196 are classified as hate speech, 168 are related to hate speech but depends on content and 73 are non English sentences, classified as idk/skip. For the purpose of this research, relation and idk/skip classes will be dropped completely in order to make the problem a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep sentences classified only as noHate and hate\n",
    "cleanPosts = annot[(annot['label'] != 'relation') & (annot['label'] != 'idk/skip')]\n",
    "# Reset index\n",
    "cleanPosts=cleanPosts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 data points\n",
    "cleanPosts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 data points\n",
    "cleanPosts.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update STOPWORDS\n",
    "stop_words = STOPWORDS.update([\"https\", \"S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as hate\n",
    "wc_hate = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts['message'][cleanPosts['label']=='hate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_hate)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as noHate\n",
    "wc_noHate = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts['message'][cleanPosts['label']=='noHate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_noHate)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating Paper Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process text with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I proceed with the Bag of Words model, I need to pre-process the text (remove punctuations, stop words, lemmatize) in order to reduce the vocabulary size. I will use SpaCy for text pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English words from NLTK\n",
    "#nltk.download('words')\n",
    "#words = set(nltk.corpus.words.words())\n",
    "# Instantiate SpaCy's English module\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Remove default stop words and lemmatize each sentence\n",
    "cleanPosts['clean_message'] = cleanPosts.message.apply(\n",
    "    lambda text: \n",
    "        \" \".join(\n",
    "            token.lemma_.lower() for token in nlp(text)\n",
    "                if token.lemma_.lower() not in nlp.Defaults.stop_words and token.is_alpha\n",
    "                 #and token.lemma_.lower() in words\n",
    "        )\n",
    ")\n",
    "# Drop old sentences\n",
    "cleanPosts.drop(columns='message', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 data points\n",
    "cleanPosts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 data points\n",
    "cleanPosts.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty strings and remove\n",
    "cleanPosts[cleanPosts['clean_message'] == ''].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 233 empty entries after lemmatization and removing stop words, which I am going to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'clean_message' is an empty string\n",
    "# Get index\n",
    "idx = cleanPosts[cleanPosts['clean_message'] == ''].index\n",
    "# Drop rows based on index\n",
    "cleanPosts.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for cleaned posts classified as hate\n",
    "wc_hate_clean = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts['clean_message'][cleanPosts['label']=='hate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_hate_clean)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for cleaned posts classified as noHate\n",
    "wc_noHate_clean = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts['clean_message'][cleanPosts['label']=='noHate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_noHate_clean)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class distribution\n",
    "plot = cleanPosts.label.value_counts().plot(kind='pie',autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Vectorize\n",
    "X = vectorizer.fit_transform(cleanPosts.clean_message.to_list())\n",
    "X_bow = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset for SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, cleanPosts.label, test_size=0.25, random_state=42, stratify=cleanPosts.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify text using SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate SVC\n",
    "svm = LinearSVC()\n",
    "# Fit training data\n",
    "svm.fit(X_train, y_train)\n",
    "# Get prediction on test data\n",
    "pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy\n",
    "print('Test Accuracy: '+str(svm.score(X_test, y_test)))\n",
    "# Print the F1 Score\n",
    "print('F1 Score: '+str(f1_score(y_test, pred, pos_label='hate')))\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, pred, labels=svm.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the available txt files with text pre-processing on a SVM Model returned an accuracy of 0.87 but an F1 Score of 0.48. This is because there is a big class inbalance in the training data. The SVM model does a really good job at classifying correctly the noHate sentences but handles poorly the hate sentences since more than half of them are missclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the sampled train and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the provided sampled_train and sampled_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled Training Data\n",
    "# Load the annotations metadata csv to get label info\n",
    "annot_tr = pd.read_csv(\"hate-speech-dataset-master/annotations_metadata.csv\")\n",
    "# Keep only file id and label\n",
    "annot_tr.drop(columns=['user_id','subforum_id','num_contexts'],inplace=True)\n",
    "# Add new empty column to insert the message\n",
    "annot_tr['message']=''\n",
    "# Loop through all lines\n",
    "for i in range(len(annot_tr['file_id'])):\n",
    "    # Get current filename\n",
    "    filename = annot_tr['file_id'][i]\n",
    "    try:\n",
    "        # Open the file and read the contents\n",
    "        with open('hate-speech-dataset-master/sampled_train/' +filename +'.txt') as f:\n",
    "            contents= f.read()\n",
    "        f.close()\n",
    "        # Add contents to the message column\n",
    "        annot_tr['message'][i]=contents\n",
    "    except:\n",
    "        print('File '+str(filename)+' not in train samples.')\n",
    "# Drop file id\n",
    "annot_tr.drop(columns=['file_id'],inplace=True)\n",
    "\n",
    "# Sampled Test Data\n",
    "# Load the annotations metadata csv to get label info\n",
    "annot_ts = pd.read_csv(\"hate-speech-dataset-master/annotations_metadata.csv\")\n",
    "# Keep only file id and label\n",
    "annot_ts.drop(columns=['user_id','subforum_id','num_contexts'],inplace=True)\n",
    "# Add new empty column to insert the message\n",
    "annot_ts['message']=''\n",
    "# Loop through all lines\n",
    "for i in range(len(annot_ts['file_id'])):\n",
    "    # Get current filename\n",
    "    filename = annot_ts['file_id'][i]\n",
    "    try:\n",
    "        # Open the file and read the contents\n",
    "        with open('hate-speech-dataset-master/sampled_test/' +filename +'.txt') as f:\n",
    "            contents= f.read()\n",
    "        f.close()\n",
    "        # Add contents to the message column\n",
    "        annot_ts['message'][i]=contents\n",
    "    except:\n",
    "        print('File '+str(filename)+' not in test samples.')\n",
    "# Drop file id\n",
    "annot_ts.drop(columns=['file_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep sentences classified only as noHate and hate - Train Sample\n",
    "cleanPosts_tr = annot_tr[(annot_tr['label'] != 'relation') & (annot_tr['label'] != 'idk/skip')]\n",
    "# Reset index\n",
    "cleanPosts_tr = cleanPosts_tr.reset_index(drop=True)\n",
    "\n",
    "# Keep sentences classified only as noHate and hate - Test Sample\n",
    "cleanPosts_ts = annot_ts[(annot_ts['label'] != 'relation') & (annot_ts['label'] != 'idk/skip')]\n",
    "# Reset index\n",
    "cleanPosts_ts = cleanPosts_ts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCloud for Posts Classified as Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update STOPWORDS\n",
    "stop_words = STOPWORDS.update([\"https\", \"S\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as hate - Train Sample\n",
    "wc_hate_tr = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts_tr['message'][cleanPosts_tr['label']=='hate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_hate_tr)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as hate - Test Sample\n",
    "wc_hate_ts = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts_ts['message'][cleanPosts_ts['label']=='hate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_hate_ts)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCloud for Posts Classified as noHate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as noHate - Train Sample\n",
    "wc_noHate_tr = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts_tr['message'][cleanPosts_tr['label']=='noHate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_noHate_tr)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud for posts classified as noHate - Test Sample\n",
    "wc_noHate_ts = WordCloud(stopwords=STOPWORDS, background_color='white').generate(' '.join(cleanPosts_ts['message'][cleanPosts_ts['label']=='noHate']))\n",
    "# Generate plot\n",
    "plt.imshow(wc_noHate_ts)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating Paper Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process text with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SpaCy's English module - Train Sample\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Remove default stop words and lemmatize each sentence\n",
    "cleanPosts_tr['clean_message'] = cleanPosts_tr.message.apply(\n",
    "    lambda text: \n",
    "        \" \".join(\n",
    "            token.lemma_.lower() for token in nlp(text)\n",
    "                if token.lemma_.lower() not in nlp.Defaults.stop_words and token.is_alpha\n",
    "        )\n",
    ")\n",
    "# Drop old sentences\n",
    "cleanPosts_tr.drop(columns='message', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SpaCy's English module - Test Sample\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Remove default stop words and lemmatize each sentence\n",
    "cleanPosts_ts['clean_message'] = cleanPosts_ts.message.apply(\n",
    "    lambda text: \n",
    "        \" \".join(\n",
    "            token.lemma_.lower() for token in nlp(text)\n",
    "                if token.lemma_.lower() not in nlp.Defaults.stop_words and token.is_alpha\n",
    "        )\n",
    ")\n",
    "# Drop old sentences\n",
    "cleanPosts_ts.drop(columns='message', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'clean_message' is an empty string - Train Sample\n",
    "# Get index\n",
    "idx = cleanPosts_tr[cleanPosts_tr['clean_message'] == ''].index\n",
    "# Drop rows based on index\n",
    "cleanPosts_tr.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'clean_message' is an empty string - Train Sample\n",
    "# Get index\n",
    "idx = cleanPosts_ts[cleanPosts_ts['clean_message'] == ''].index\n",
    "# Drop rows based on index\n",
    "cleanPosts_ts.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class distribution - Train\n",
    "plot_tr = cleanPosts_tr.label.value_counts().plot(kind='pie',autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class distribution - Test\n",
    "plot_ts = cleanPosts_ts.label.value_counts().plot(kind='pie',autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine train and test samples into one pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train and test samples to use for the BoW vocabulary\n",
    "cleanPosts_cb = pd.concat([cleanPosts_tr, cleanPosts_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Create the vocabulary\n",
    "vectorizer.fit(cleanPosts_cb.clean_message.to_list())\n",
    "\n",
    "# Transform train sample to document-term matrix\n",
    "X_tr = vectorizer.transform(cleanPosts_tr.clean_message.to_list())\n",
    "# Create a dataframe from the document-term matrix\n",
    "X_train_b = pd.DataFrame(X_tr.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Transform test sample to document-term matrix\n",
    "X_ts = vectorizer.transform(cleanPosts_ts.clean_message.to_list())\n",
    "# Create a dataframe from the document-term matrix\n",
    "X_test_b = pd.DataFrame(X_ts.toarray(),columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify text using the SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate SVC\n",
    "svm = LinearSVC()\n",
    "# Fit training data\n",
    "svm.fit(X_train_b, cleanPosts_tr.label)\n",
    "# Get prediction on test data\n",
    "pred = svm.predict(X_test_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Accuracy, F1 Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy\n",
    "print('Accuracy: '+str(svm.score(X_test_b, cleanPosts_ts.label)))\n",
    "# Print the F1 Score\n",
    "print('F1 Score: '+str(f1_score(cleanPosts_ts.label, pred, pos_label='hate')))\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(cleanPosts_ts.label, pred, labels=svm.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sampled train and test data returned an accuracy of 0.74, which is lower than the accuracy I got using the whole dataset. However, the F1 Score is much higher, 0.74, and the model does a fair job in correctly classifying both hate and noHate classes. These results are very close to the results from the paper and indicate the importance of having a class balaned dataset when it comes to text classification. For the rest of this research, the balanced train and test datasets will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify text using CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hate Speech Dataset paper provides the link for the code used for the CNN model. The code below was downloaded from https://github.com/dennybritz/cnn-text-classification-tf and https://github.com/lubiluk/cnn-sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the dataset\n",
    "1) Tokenize the text into number sequences\n",
    "2) Pad sequences to the same length (of the longest sequence)\n",
    "3) One hot encode train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from notebook provided in https://github.com/lubiluk/cnn-sentence\n",
    "# Build the vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in cleanPosts_cb.clean_message.to_list()])\n",
    "# Tokenizer default filtering seems to be identical to original clean_str\n",
    "tokenizer = Tokenizer()\n",
    "# Fit vocabulary using both train and test\n",
    "tokenizer.fit_on_texts(cleanPosts_cb.clean_message.to_list())\n",
    "\n",
    "# Convert train to sequences\n",
    "x_train_cnn = np.array(tokenizer.texts_to_sequences(cleanPosts_tr.clean_message.to_list()), dtype='object')\n",
    "# Pad examples to the same length\n",
    "x_train_cnn = keras.preprocessing.sequence.pad_sequences(x_train_cnn,\n",
    "                                               value=0,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_document_length)\n",
    "\n",
    "# Convert test to sequences\n",
    "x_test_cnn = np.array(tokenizer.texts_to_sequences(cleanPosts_ts.clean_message.to_list()), dtype='object')\n",
    "# Pad examples to the same length\n",
    "x_test_cnn = keras.preprocessing.sequence.pad_sequences(x_test_cnn,\n",
    "                                               value=0,\n",
    "                                               padding='post',\n",
    "                                               maxlen=max_document_length)\n",
    "\n",
    "# one hot encode train labels\n",
    "y_train_cnn = pd.get_dummies(cleanPosts_tr.label).astype('int32').values\n",
    "\n",
    "# one hot encode test labels\n",
    "y_test_cnn = pd.get_dummies(cleanPosts_ts.label).astype('int32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of the training data to use for validation\n",
    "dev_sample_percentage = 0.1\n",
    "# Dimensionality of character embedding (default: 128)\n",
    "embedding_dim = 128\n",
    "# Filter sizes (default: [3, 4, 5])\n",
    "filter_sizes = [3, 4, 5]\n",
    "# Number of filters per filter size (default: 128)\n",
    "num_filters = 128\n",
    "# L2 regularization lambda (default: 0.0) (unused)\n",
    "l2_reg_lambda = 0.0\n",
    "# Dropout keep probability (default: 0.5)\n",
    "dropout_keep_prob = 0.5\n",
    "# Batch Size (default: 64)\n",
    "batch_size = 32\n",
    "# Number of training epochs (default: 200)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "sequence_length = x_train_cnn.shape[1]\n",
    "num_classes = y_train_cnn.shape[1]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = embedding_dim\n",
    "\n",
    "print(\"Sequence Length: {}\".format(sequence_length))\n",
    "print(\"Number of Classes: {}\".format(num_classes))\n",
    "print(\"Vocabulary Size: {}\".format(vocab_size))\n",
    "print(\"Embedding Size: {}\".format(embedding_size))\n",
    "\n",
    "inputs = keras.layers.Input(shape=(sequence_length,))\n",
    "x = keras.layers.Embedding(vocab_size, embedding_size, input_length=sequence_length,\n",
    "                          embeddings_initializer='random_uniform')(inputs)\n",
    "x = keras.layers.Reshape((sequence_length, embedding_size, 1))(x)\n",
    "\n",
    "filter_layers = []\n",
    "\n",
    "for s in filter_sizes:\n",
    "  f = keras.layers.Conv2D(\n",
    "      num_filters, kernel_size=(s, embedding_size), strides=(1, 1),\n",
    "      padding='valid', data_format='channels_last', activation='relu',\n",
    "      use_bias=True, kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.01),\n",
    "      bias_initializer=keras.initializers.Constant(value=0.1))(x)\n",
    "  f = keras.layers.MaxPooling2D(pool_size=(sequence_length - s + 1, 1), strides=(1, 1), padding='valid', data_format='channels_last')(f)\n",
    "  filter_layers.append(f)\n",
    "\n",
    "# Combine all the pooled features\n",
    "x = keras.layers.concatenate(filter_layers, axis=1)\n",
    "x = keras.layers.Reshape((num_filters * len(filter_sizes), ))(x)\n",
    "\n",
    "# Add dropout\n",
    "x = keras.layers.Dropout(dropout_keep_prob)(x)\n",
    "\n",
    "# Final (unnormalized) scores and predictions\n",
    "predictions = keras.layers.Dense(num_classes, use_bias=True, \n",
    "                                 bias_initializer=keras.initializers.Constant(value=0.1),\n",
    "                                 kernel_initializer=keras.initializers.glorot_uniform(seed=None),\n",
    "                                 kernel_regularizer=keras.regularizers.l2(l2_reg_lambda), \n",
    "                                 bias_regularizer=keras.regularizers.l2(l2_reg_lambda),\n",
    "                                 activation='softmax')(x)\n",
    "  \n",
    "model = keras.Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify loss function and optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_cnn,\n",
    "                    y_train_cnn,\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=dev_sample_percentage,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
